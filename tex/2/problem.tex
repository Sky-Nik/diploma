\section{Задача}

\begin{problem}
    Візьмемо $F(x) = M x + q$, де матриця $M$ генерується наступний чином:
    \begin{equation}
        M = A A^\intercal + B + D,
    \end{equation}
    де всі елементи $m \times m$ матриці $A$ і $m \times m$ кососиметричної матриці $B$ обираються рівномірно випадково з $(-5, 5)$, а усі елементи діагональної матриці $D$ вибираються рівномірно випадково з $(0, 0.3)$ (як наслідок, матриця $M$ додатно визначена), а кожен елемент $q$ обирається рівномірно випадково з $(-500, 0)$. Допустимою множиною є 
    \begin{equation}
        C = \left\{ x \in \RR_+^m \middle| x_1 + x_2 + \dots + x_m = m \right\},
    \end{equation}
    а за початкове наближення береться $x_1 = (1, \dots, 1)$. Для цієї задачі $L = \no{M}$, $\epsilon = 10^{-3}$.
\end{problem}

\subsection{Алгоритм проектування}

% Допустима множина цієї задачі --- так званий \emph{probability symplex} (з точністю до константи $m$). Для проектування $\vec y$ на нього ми використовували наступний явний
\begin{algorithm}\nothing

    \textbf{Крок 1.} Відсортувати елементи $\vec y$ і зберегти в $\vec u$: $u_1 \ge \dots \ge u_m$. \medskip
        
    \textbf{Крок 2.} Знайти $k = \max j$: $u_j + \frac{1}{j} \left( m - \Sum_{i = 1}^j u_i \right) > 0$. \medskip
        
    \textbf{Крок 3.} Видати вектор з елементами $x_i = \max\{y_i + \lambda, 0\}$, $\lambda = \frac{1}{k} \left( m - \Sum_{i = 1}^k u_i \right)$.
\end{algorithm}

\emph{Цей алгоритм взято із статті} \href{https://arxiv.org/pdf/1309.1541v1.pdf}{[Weiran Wang, Miguel A. Carreira-Perpi\~n\'an, 2013]}, хоча він зустрічається у літературі і раніше.

\subsection{Приклад клієнтського коду}

\begin{minted}[linenos,fontsize=\tiny]{python}
def ProjectionOntoProbabilitySymplex(x: np.array) -> np.array:
    dimensionality = x.shape[0]
    x /= dimensionality
    sorted_x = np.flip(np.sort(x))
    prefix_sum = np.cumsum(sorted_x)
    to_compare = sorted_x + (1 - prefix_sum) / np.arange(1, dimensionality + 1)
    k = 0
    for j in range(1, dimensionality): if to_compare[j] > 0: k = j
    return dimensionality * np.maximum(np.zeros(dimensionality), x + (to_compare[k] - sorted_x[k]))

solution, iteration_n, duration = korpelevich(...
    A=lambda x: M.dot(x) + q,
    ProjectionOntoC=ProjectionOntoProbabilitySymplex, ...)
\end{minted}